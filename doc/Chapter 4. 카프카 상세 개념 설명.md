# 아파치 카프카 애플리케이션 프로그래밍 with 자바
# Chapter 4. 카프카 상세 개념 설명

## 4.1 토픽과 파티션

### 4.1.1 적정 파티션 개수

* 토픽의 파티션 개수는 카프카의 성능과 관련이 있기에, 적절한 파티션의 개수를 설정하고 운영하는 것이 중요.
* 토픽 생성시 파티션 개수 고려사항 3가지

	* 1) 데이터 처리량

		* 파티션은 카프카의 병럴처리의 핵심이며, 파티션의 개수가 많아지면 많아질수록 1:1 매핑되는 컨슈머의 개수가 늘어남.
		* 따라서 파티션 개수를 정할 때는 해당 토픽에 필요한 데이터 처리량을 측정하여 정하는 것이 중요함.

		* 데이터 처리 속도를 올리는 방법
			a) 컨슈머의 처리량을 늘리는 것
			   - 컨슈머가 실행되는 서버의 사양을 올리는 스케일 업을 하거나, GC 튜닝 등을 활용.
			     하지만, 컨슈머 특성상 다른 시스템들(S3, 하듑, 오라클 등)과 연동되기 때문에 일정 수준 이상 처리량을 올리는 것은 어려움.
			b) 컨슈머를 추가하여 병렬처리량을 늘리는 것.
			   - 파티션 개수를 늘리고 파티션 개수만큼 컨슈머를 추가하는 방법은 데이터 처리량을 늘리는 가장 확실항 방법.
			     프로듀서가 보내는 데이터양과 컨슈머의 데이터 처리량을 계산하여 파티션 개수를 정하면 됨.
			   - 파티션 개수만큼 컨슈머 스레드를 운영한다면 해당 토픽의 병렬 처리를 극대화할 수 있음.
			     만약 전체 컨슈머 데이터 처리량이 프로듀서가 보내는 데이터보다 적다면 컨슈머 랙이 생기고, 데이터 처리 지연이 발생함.
			     따라서 컨슈머 전체 데이터 처리량이 프로듀서 데이터 처리량보다 많아야 함.
			  => 프로듀서 전송 데이터 양 < (컨슈머 데이터 처리 양 X 파티션 개수)

		* 컨슈머 데이터 처리량을 구하는 방법?
		  -> 상용에서 운영중인 카프카에서 더미 데이터로 테스트를 해보는 것.
			 카프카 클러스터와 다른 시스템과 연동되기 때문에 로컬 또는 테스트환경에서의 측정이 한계가 있음.

		* 프로듀서가 보내는 데이터양?
		  -> 하루, 시간, 분 단위로 쪼개어 예측함.
		     데이터의 지연이 절대 발생되서는 안된다면 프로듀서가 보내는 데이터의 최대치를 데이터 생성량으로 잡고 계산.

			 하지만 파티션 개수를 무조건 늘리면 컨슈머와 브로커의 부담이 있기 때문에,
			 지연 발생에 따른 서비스 영향도를 고려하여 파티션 개수를 구하는 것이 중요함.

	* 2) 메시지 키 사용 여부

		* 메시지 키를 사용함과 동시에 데이터 처리 순서를 지켜야 한다.

		* 프로듀서가 기본 파티셔너를 사용하는 경우를 가정했을때, 
		  메시지 키를 사용하면 프로듀서가 토픽으로 데이터를 보낼 때 메시지 키를 해시 변환하여 메시지 키를 파티션에 매칭시킴.
		  만약 파티션 개수가 달라지면 이미 매칭된 파티션과 메시지 키의 매칭이 깨지고 전혀 다른 파티션에 데이터가 할당됨.
		  -> 파티션 개수가 달라지는 순간에는 메시지 키를 사용하는 컨슈머는 특정 메시지 키의 순서를 더는 보장받지 못함.
			 파티션을 변환하기 이전과 이후 메시지 키의 파티션 위치가 달라지기 때문.

		* 메시지 키를 사용하고 컨슈머에서 메시지 처리 순서가 보장되어야 한다면 최대한 파티션의 변화가 발생하지 않도록 운영해야 함.
		  만약 파티션 개수가 변해야 하는 경우에는, 기존에 사용하던 메시지 키의 매칭을 그대로 가져가기 위해 커스텀 파티셔너를 개발하고 적용해야 함.

		* 따라서 메시지 키별로 처리 순서를 보장하기 위해서는 파티션 개수를 프로듀서가 전송하는 데이터양보다 더 넉넉하게 잡고 생성하는 것이 좋음.
		  반면, 메시지 키를 사용하지만 데이터 처리 순서를 지키지 않아도 된다면 파티션 개수를 처음부터 넉넉하게 잡지 않아도 됨.
		  데이터의 양에 따라 파티션을 늘리면 되기 때문.

	* 3) 브로커, 컨슈머 영향도

		* 카프카에서 파티션은 각 브로커의 파일 시스템을 사용하기 때문에 파티션이 늘어나는 만큼 브로커에서 접근하는 파일 개수가 많아짐.
		  운영체제에서는 프로세스당 열 수 있는 파일의 최대 개수를 제한하고 있으므로, 
		  카프카 브로커가 접근하는 파일 개수를 안정적으로 유지하기 위해서는 각 브로커당 파티션 개수를 모니터링 해야 함.

		* 데이터양이 많아져서 파티션 개수를 늘려야 하는 상황이라면 브로커당 파티션 개수를 확인하고 진행해야 하며,
		  만약 브로커가 관리하는 파티션 개수가 너무 많으면 파티션 개수를 분산하기 위해 카프카 브로커 개수를 늘리는 방안도 고려해야 함.


### 4.1.2 토픽 정리 정책(cleanup.policy)

* 토픽의 데이터는 시간 또는 용량에 따라 삭제 규칙을 적용할 수 있음.
  삭제를 원하지 않는다면 카프카 클러스터가 살아있는 한 토픽의 데이터를 삭제하지 않도록 설정할 수 있음.
* 용량이 늘어나면 운영 비용도 늘어나기 때문에 데이터를 사용하지 않을때엔 cleanup.policy 옵션을 사용하여 데이터를 삭제함.

* 토픽 삭제 정책(delete policy)

	토픽의 데이터를 삭제할 때엔 세그먼트 단위로 삭제를 진행함. 
	세그먼트란, 토픽의 데이터를 저장하는 명시적인 파일 시스템 단위.
	세그먼트는 파티션마다 별개로 생성되며, 세그먼트의 파일 이름은 오프셋중 가장 작은 값이 됨.
	세그먼트는 여러 조각으로 나뉘는데, segment.bytes 옵션으로 1개의 세그먼트 크기를 지정할 수 있음.
	이 크기보다 커지면 세그먼트 파일을 닫고 새로운 세그먼트를 열어서 파일을 저장하는데, 이 때 사용중인 세그먼트를 액티브 세그먼트라고 함.

	삭제 정책이 실행되는 시점은 시간 또는 용량이 기준이 됨.
	retention.ms는 토픽의 데이터를 유지하는 기간을 ms로 설정할 수 있음.
	카프카는 일정 주기마다 세그먼트 파일의 수정시간과 retention.ms 를 비교하여 삭제시킴.
	retention.bytes는 토픽의 최대 데이터 크기를 제어함.
	retention.bytes를 넘어간 세그먼트 파일은 삭제되며,
	한 번 삭제된 데이터는 복구할 수 없음.


* 토픽 압축 정책(compact policy)

	토픽의 압축 정책은 일반적으로 생각하는 zip 압축과는 다른 개념이며,
	여기서 압축이란 메시지 키별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책을 뜻함.

	메시지 키를 기준으로 오래된 데이터를 삭제하기 때문에 삭제정책과 다르게 1개 파티션에서 오프셋의 증가가 일정하지 않을 수 있음.
	1~10까지의 오프셋 중 4,5,6 이 동일한 메시지 키를 가지고 있다고 해도, 4,5만 삭제될 수 있음.
	동일한 키를 가지고 있어도 4,5 오프셋의 레코드가 6보다 오래됐기 때문.

	데이터의 흐름이 아닌 가장 마지막으로 업데이트된 메시지 키의 데이터가 중요할 경우, 가장 최신의 데이터를 제외한 나머지 데이터를 삭제할 수 있음.

	압축 정책은 액티브 세그먼트를 제외한 나머지 세그먼트들에 한해서만 데이터를 처리함.
	데이터의 압축 시작 시점은 min.cleanable.dirty.ratio 옵션값을 따름.
	위 옵션값은 액티브 세그먼트를 제외한 세그먼트에 남아 있는 데이터의 tail 영역의 레코드 개수와 head 영역의 레코드 개수의 비율을 뜻함.

	tail 영역은 압축 정책에 의해 압축이 완료된 레코드를 뜻함. 
	tail 영역의 레코드들은 '클린 로그'라고 부르며, 압축이 완료됐기 때문에 tail 영역에는 중복된 메시지 키가 없음.

	head 영역의 레코드들은 '더티 로그'라고 부르며, 압축되기 전 레코드들이 있으므로 중복된 메시지 키를 가진 레코드들이 있음.

	'더티 비율'은 더티 영역의 메시지 개수를 압축 대상 세그먼트에 남아있는 데이터의 총 레코드 수(더티 영역 메시지 개수+클린 영역 메시지 개수)로 나눈 비율.

	토픽의 압축은 min.cleanable.dirty.ratio 값에 따라 수행됨.
	해당 값을 넘어가는 순간 압축이 수행됨. 해당 값을 0.9 로 셋팅하면 한 번 압축할 때 많은 데이터가 줄어드므로 압축 효과가 좋지만,
	비율이 0.9에 도달하기 전까지 용량을 차지하므로 용량 효율이 좋지 않음.

	반대로 0.1과 같이 작게 설정하면 압축이 자주 일어나기 때문에 메시지 키의 최신 데이터만 유지할 수 있지만,
	압축이 자주 일어나는 만큼 브로커에 부담을 줄 수 있음.

	적절한 값을 찾는 것이 중요함.

### 4.1.3 ISR(In-Sync-Replicas)

* ISR은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻함.
  (리더 파티션의 모든 데이터가 팔로워 파티션에 복제된 상태를 말함)

* 프로듀서가 특정 파티션에 데이터를 저장하는 작업은 리더 파티션을 통해 처리하는데,
  리더 파티션에 새로운 레코드가 추가되어 오프셋이 증가하면 팔로워 파티션이 위치한 브로커는 리더 파티션의 데이터를 복제함.
  복제하는 시간차 때문에 리더 파티션과 팔로워 파티션 간에 오프셋 차이가 발생하며,
  이런 차이를 모니터링하기 위해 리더 파티션은 replica.lag.time.max.ms값 만큼의 주기를 가지고 팔로워 파티션의 데이터 복제여부를 확인함.
  설정 시간보다 더 긴 시간동안 데이터를 가져가지 않는다면 해당 팔로워 파티션에 문제가 생긴 것으로 판단하고 ISR 그룹에서 제외함.

* ISR로 묶인 리더&팔로워 파티션은 파티션에 존재하는 데이터가 모두 동일하기 때문에 팔로워 파티션은 리더 파티션으로 선출될 자격을 가짐.

* 데이터 유실되더라도 서비스를 중단하지 않고 토픽을 지속적으로 사용하고 싶다면 ISR이 아닌 팔로워 파티션을 리더로 선출할 수 있도록 설정이 가능함.
  unclean.leader.election.enabel 옵션을 true로 설정하면 ISR이 아닌 팔로워 파티션도 리더가 될 수 있음.
  동기화되지 않은 상태이므로 데이터 유실될 수 있음.


## 4.2 카프카 프로듀서

### 4.2.1 acks 옵션

* acks 옵션을 통해 프로듀서가 전송한 데이터가 카프카 클러스터에 얼마나 신뢰성 높게 저장할지 지정할 수 있음.

	1) acks = 0

		프로듀서가 리더 파티션으로 데이터를 전송했을 때 리더 파티션으로 데이터가 저장되었는지 확인하지 않는다는 뜻.
		리더 파티션은 데이터가 저장된 이후, 몇번째 오프셋에 저장했는지 리턴하는데 acks가 0으로 설정되어있다면,
		프로듀서는 리더 파티션에 데이터가 저장되었는지 여부에 대한 응답값을 받지 않음.

		프로듀서에는 데이터 전송 실패시 retries 옵션을 통해 재시도를 할 수 있도록 설정할 수 있는데,
		acks를 0일때 프로듀서는 전송하자마자 데이터가 저장되었음을 가정하고 다음 데이터를 전송하기 때문에
		전송 성공/실패 여부를 알 수 없으므로, retries 옵션은 무의미함.

		데이터 전송 속도는 acks가 1이나 all일 때보다 훨씬 빠르므로, 데이터 유실되더라도 빠른 속도를 보장해야 할 때 이 옵션을 사용.

	2) acks = 1

		리더 파티션에만 정상적으로 적재되었는지 확인하는 옵션값.
		하지만 팔로워 파티션이 데이터를 복제(동기화)했는지 알 수 없기 때문에 리더 파티션이 있는 브로커가 장애나면 데이터 유실될 수 있음.

	3) acks = all 또는 acks = -1

		리더 파티션과 팔로워 파티션에 모두 정상적으로 적재되었는지 확인하기 때문에 다른 옵션보다 속도는 느리지만 데이터는 안전함.
		all로 설정한 경우, 토픽 단위로 설정 가능한 min.insync.replicas 옵션값에 따라 데이터의 안정성이 달라짐.

		> all 옵션값은 모든 파티션에 적재했다는 것을 뜻하는 것이 아니고, ISR 에 포함된 파티션을 의미함 <

		min.insync.replicas 옵션은 프로듀서가 리더 파티션과 팔로워 파티션에 데이터가 적재되었는지 확인하기 위한 최소 ISR 그룹의 파티션 개수.
		min.insync.replicas 을 1로 설정하면, acks를 1로 설정한 것과 동일함. 
		(가장 처음 적재되는 파티션이 리더 파티션이기 때문.)
		min.insync.replicas을 2로 설정할 때 부터 acks=all 로 설정한 의미가 있음.

		min.insync.replicas 를 설정할 때엔 복제 개수도 고려해야 함.
		운영하는 브로커 개수가 위 옵션 값보다 작은 경우에는 프로듀서가 데이터를 전송할 수 없음.
		브로커 개수와 동일한 옵션값을 주었다가 브로커 1대가 이슈가 발생하면 프로듀서는 데이터를 해당 토픽에 더이상 전송할 수 없는 상황이 발생함.

		따라서 토픽별 min.insync.replicas 옵션값은 브로커 개수 미만으로 설정해서 운영해야 함. (같아도 안됨..)
		일반적으로 브로커 3대 이상으로 묶어 클러스터를 운영하는데, 토픽의 복제개수는 3, min.insync.replicas 는 2로 설정하고
		프로듀서는 acks를 all로 설정하는 것을 추천.


### 4.2.2 멱등성(idempotence) 프로듀서

* 멱등성이란 여러 번 연산을 수행하더라도 동일한 결과를 나타내는 것을 뜻함.
  카프카에서는 동일한 데이터를 여러 번 전송하더라도 카프카 클러스터에 단 한 번만 저장됨을 의미.
  enable.idempotence 옵션을 사용하면 정확히 한번 전달할 수 있음.
  멱등성 프로듀서는 프로듀서 PID(Producer unique ID)와 Sequencs number 를 함께 전달함.

* 애플리케이션을 재시작하면 PID가 달라져 다른 데이터로 인식하는 것 주의.

* 멱등성 프로듀서를 사용하기 위해 enable.idempotence 를 true 로 설정하면,
  프로듀서의 일부 옵션들이 강제로 설정됨.
  retries 는 기본값으로 Integer.MAX_VALUE 로 설정되고, acks 옵션은 all로 설정됨.
  프로듀서가 적어도 한 번 이상 브로커에 데이터를 보냄으로써 단 한 번만 데이터가 적재되는 것을 보장하기 위함.

* 멱등성 프로듀서는 정확히 한 번만 데이터를 전송하는 것이 아니라, 중복된 데이터를 적재하지 않는 것임.


### 4.2.3 트랜잭션(transaction) 프로듀서

* 카프카의 트랜잭션 프로듀서는 다수의 파티션에 데이터를 저장할 경우 모든 데이터에 대해 동일한 원자성(atomic)을 만족시키기 위해 사용됨.
  다수의 데이터를 동일 트랜잭션으로 묶음으로써 전체 데이터를 처리하거나 전체 데이터를 처리하지 않도록 하는 것을 의미.

* 컨슈머는 기본적으로 프로듀서가 보내는 데이터가 파티션에 쌓이는대로 모두 가져가서 처리하지만,
  트랜잭션으로 묶인 데이터를 브로커에서 가져갈 때는 다르게 동작하도록 설정할 수 있음.

* 트랜잭션 프로듀서를 사용하려면 enable.idempotence 를 true로 설정하고 transactional.id 를 임의의 String 값으로 정의하고,
  컨슈머의 isolation.level 을 read_committed 로 설정하면
  프로듀서와 컨슈머는 트랜잭션으로 처리 완료된 데이터만 쓰고 읽게 됨.

* 트랜잭션은 파티션의 레코드로 구분함.
  트랜잭션 프로듀서는 사용자가 보낸 데이터를 레코드 파티션에 저장함과 동시에 트랜잭션의 시작과 끝을 표현하기 위한 트랜잭션 레코드를 한 개 더 보냄.
  실질적인 데이터는 가지고 있지 않으며, 트랜잭션이 끝난 상태를 표시하는 정보만 가지고 있음.
  대신 레코드의 특성은 그대로 가지고 있기 때문에 파티션에 저장되어 오프셋을 한 개 차지함.

* 트랜잭션 컨슈머는 커밋이 완료된 데이터가 파티션에 있을 경우에만 데이터를 가져감.
